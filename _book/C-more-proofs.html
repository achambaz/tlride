<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>C More results and their proofs | A Ride in Targeted Learning Territory</title>
  <meta name="description" content="To do…" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="C More results and their proofs | A Ride in Targeted Learning Territory" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="cover.jpg" />
  <meta property="og:description" content="To do…" />
  <meta name="github-repo" content="achambaz/tlride" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="C More results and their proofs | A Ride in Targeted Learning Territory" />
  
  <meta name="twitter:description" content="To do…" />
  <meta name="twitter:image" content="cover.jpg" />

<meta name="author" content="David Benkeser (Emory University)" />
<meta name="author" content="Antoine Chambaz (Université de Paris)" />


<meta name="date" content="2020-05-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
<link rel="prev" href="B-proofs.html"/>
<link rel="next" href="D-references.html"/>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  CommonHTML: {
    scale: 90,
    linebreaks: {
      automatic: true
    }
  },
  SVG: {
    linebreaks: {
      automatic: true
    }
  }, 
  displayAlign: "left"
  });
</script>
<script type="text/javascript"
	src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script><!-- see also '_output.yaml'
src="https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"
src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML" 
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
-->


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="tlride.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="https://achambaz.github.io/tlride/">TLRIDE</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="part"><span><b>I On the road</b></span></li>
<li class="chapter" data-level="1" data-path="1-a-ride.html"><a href="1-a-ride.html"><i class="fa fa-check"></i><b>1</b> A ride</a><ul>
<li class="chapter" data-level="1.1" data-path="1-a-ride.html"><a href="1-a-ride.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-a-ride.html"><a href="1-a-ride.html#causal-story"><i class="fa fa-check"></i><b>1.1.1</b> A causal story</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-a-ride.html"><a href="1-a-ride.html#tlrider-package"><i class="fa fa-check"></i><b>1.1.2</b> The <code>tlrider</code> package</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-a-ride.html"><a href="1-a-ride.html#discuss"><i class="fa fa-check"></i><b>1.1.3</b> What we will discuss</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-a-ride.html"><a href="1-a-ride.html#simulation-study"><i class="fa fa-check"></i><b>1.2</b> A simulation study</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-a-ride.html"><a href="1-a-ride.html#reproducible-experiment"><i class="fa fa-check"></i><b>1.2.1</b> Reproducible experiment as a law</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-a-ride.html"><a href="1-a-ride.html#synthetic-experiment"><i class="fa fa-check"></i><b>1.2.2</b> A synthetic reproducible experiment</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-a-ride.html"><a href="1-a-ride.html#revealing-experiment"><i class="fa fa-check"></i><b>1.2.3</b> Revealing <code>experiment</code></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-a-ride.html"><a href="1-a-ride.html#exo-visualization"><i class="fa fa-check"></i><b>1.3</b> ⚙ Visualization</a></li>
<li class="chapter" data-level="1.4" data-path="1-a-ride.html"><a href="1-a-ride.html#exo-make-own-experiment"><i class="fa fa-check"></i><b>1.4</b> ⚙ Make your own experiment</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-parameter.html"><a href="2-parameter.html"><i class="fa fa-check"></i><b>2</b> The parameter of interest</a><ul>
<li class="chapter" data-level="2.1" data-path="2-parameter.html"><a href="2-parameter.html#parameter-first-pass"><i class="fa fa-check"></i><b>2.1</b> The parameter of interest</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-parameter.html"><a href="2-parameter.html#definition"><i class="fa fa-check"></i><b>2.1.1</b> Definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-parameter.html"><a href="2-parameter.html#causal-interpretation"><i class="fa fa-check"></i><b>2.1.2</b> A causal interpretation</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-parameter.html"><a href="2-parameter.html#causal-computation"><i class="fa fa-check"></i><b>2.1.3</b> A causal computation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-parameter.html"><a href="2-parameter.html#exo-alternative-parameter-first-pass"><i class="fa fa-check"></i><b>2.2</b> ⚙ An alternative parameter of interest</a></li>
<li class="chapter" data-level="2.3" data-path="2-parameter.html"><a href="2-parameter.html#parameter-second-pass"><i class="fa fa-check"></i><b>2.3</b> The statistical mapping of interest</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-parameter.html"><a href="2-parameter.html#opening"><i class="fa fa-check"></i><b>2.3.1</b> Opening discussion</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-parameter.html"><a href="2-parameter.html#parameter-mapping"><i class="fa fa-check"></i><b>2.3.2</b> The parameter as the value of a statistical mapping at the experiment</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-parameter.html"><a href="2-parameter.html#value-another-experiment"><i class="fa fa-check"></i><b>2.3.3</b> The value of the statistical mapping at another experiment</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-parameter.html"><a href="2-parameter.html#exo-alternative-parameter-second-pass"><i class="fa fa-check"></i><b>2.4</b> ⚙ Alternative statistical mapping</a></li>
<li class="chapter" data-level="2.5" data-path="2-parameter.html"><a href="2-parameter.html#parameter-third-pass"><i class="fa fa-check"></i><b>2.5</b> Representations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-parameter.html"><a href="2-parameter.html#yet-another"><i class="fa fa-check"></i><b>2.5.1</b> Yet another representation</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-parameter.html"><a href="2-parameter.html#rep-to-est"><i class="fa fa-check"></i><b>2.5.2</b> From representations to estimation strategies</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-parameter.html"><a href="2-parameter.html#exo-alternative-parameter-third-pass"><i class="fa fa-check"></i><b>2.6</b> ⚙ Alternative representation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-smooth.html"><a href="3-smooth.html"><i class="fa fa-check"></i><b>3</b> Smoothness</a><ul>
<li class="chapter" data-level="3.1" data-path="3-smooth.html"><a href="3-smooth.html#smooth-first-pass"><i class="fa fa-check"></i><b>3.1</b> Fluctuating smoothly</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-smooth.html"><a href="3-smooth.html#fluctuations"><i class="fa fa-check"></i><b>3.1.1</b> The <code>another_experiment</code> fluctuation</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-smooth.html"><a href="3-smooth.html#numerical-illus"><i class="fa fa-check"></i><b>3.1.2</b> Numerical illustration</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-smooth.html"><a href="3-smooth.html#exo-yet-another-experiment"><i class="fa fa-check"></i><b>3.2</b> ⚙ Yet another experiment</a></li>
<li class="chapter" data-level="3.3" data-path="3-smooth.html"><a href="3-smooth.html#smooth-second-pass"><i class="fa fa-check"></i><b>3.3</b> ☡  More on fluctuations and smoothness</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-smooth.html"><a href="3-smooth.html#smooth-second-pass-fluctuations"><i class="fa fa-check"></i><b>3.3.1</b> Fluctuations</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-smooth.html"><a href="3-smooth.html#smoothness-and-gradients"><i class="fa fa-check"></i><b>3.3.2</b> Smoothness and gradients</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-smooth.html"><a href="3-smooth.html#Euclidean-perspective"><i class="fa fa-check"></i><b>3.3.3</b> A Euclidean perspective</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-smooth.html"><a href="3-smooth.html#canonical-gradient"><i class="fa fa-check"></i><b>3.3.4</b> The canonical gradient</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-smooth.html"><a href="3-smooth.html#revisiting"><i class="fa fa-check"></i><b>3.4</b> A fresh look at <code>another_experiment</code></a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-smooth.html"><a href="3-smooth.html#deriving-the-efficient-influence-curve"><i class="fa fa-check"></i><b>3.4.1</b> Deriving the efficient influence curve</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-smooth.html"><a href="3-smooth.html#numerical-validation"><i class="fa fa-check"></i><b>3.4.2</b> Numerical validation</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-smooth.html"><a href="3-smooth.html#influence-curves"><i class="fa fa-check"></i><b>3.5</b> ☡  Asymptotic linearity and statistical efficiency</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-smooth.html"><a href="3-smooth.html#asymptotic-linearity"><i class="fa fa-check"></i><b>3.5.1</b> Asymptotic linearity</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-smooth.html"><a href="3-smooth.html#influence-curves-and-gradients"><i class="fa fa-check"></i><b>3.5.2</b> Influence curves and gradients</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-smooth.html"><a href="3-smooth.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>3.5.3</b> Asymptotic efficiency</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-smooth.html"><a href="3-smooth.html#exo-cramer-rao"><i class="fa fa-check"></i><b>3.6</b> ⚙ Cramér-Rao bounds</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-double-robustness.html"><a href="4-double-robustness.html"><i class="fa fa-check"></i><b>4</b> Double-robustness</a><ul>
<li class="chapter" data-level="4.1" data-path="4-double-robustness.html"><a href="4-double-robustness.html#linear-approximation"><i class="fa fa-check"></i><b>4.1</b> Linear approximations of parameters</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-double-robustness.html"><a href="4-double-robustness.html#from-gradients-to-estimators"><i class="fa fa-check"></i><b>4.1.1</b> From gradients to estimators</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-double-robustness.html"><a href="4-double-robustness.html#another-Euclidean-perspective"><i class="fa fa-check"></i><b>4.1.2</b> A Euclidean perspective</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-double-robustness.html"><a href="4-double-robustness.html#the-remainder-term"><i class="fa fa-check"></i><b>4.1.3</b> The remainder term</a></li>
<li class="chapter" data-level="4.1.4" data-path="4-double-robustness.html"><a href="4-double-robustness.html#expressing-the-remainder-term-as-a-function-of-the-relevant-features"><i class="fa fa-check"></i><b>4.1.4</b> Expressing the remainder term as a function of the relevant features</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-double-robustness.html"><a href="4-double-robustness.html#exo-remainder-term"><i class="fa fa-check"></i><b>4.2</b> ⚙ The remainder term</a></li>
<li class="chapter" data-level="4.3" data-path="4-double-robustness.html"><a href="4-double-robustness.html#def-double-robustness"><i class="fa fa-check"></i><b>4.3</b> ☡  Double-robustness</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-double-robustness.html"><a href="4-double-robustness.html#the-key-property"><i class="fa fa-check"></i><b>4.3.1</b> The key property</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-double-robustness.html"><a href="4-double-robustness.html#direct-consequence"><i class="fa fa-check"></i><b>4.3.2</b> Its direct consequence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-double-robustness.html"><a href="4-double-robustness.html#exo-double-robustness"><i class="fa fa-check"></i><b>4.4</b> ⚙ Double-robustness</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-inference.html"><a href="5-inference.html"><i class="fa fa-check"></i><b>5</b> Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="5-inference.html"><a href="5-inference.html#where-we-stand"><i class="fa fa-check"></i><b>5.1</b> Where we stand</a></li>
<li class="chapter" data-level="5.2" data-path="5-inference.html"><a href="5-inference.html#where-we-go"><i class="fa fa-check"></i><b>5.2</b> Where we go</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-simple-strategy.html"><a href="6-simple-strategy.html"><i class="fa fa-check"></i><b>6</b> A simple inference strategy</a><ul>
<li class="chapter" data-level="6.1" data-path="6-simple-strategy.html"><a href="6-simple-strategy.html#a-cautionary-detour"><i class="fa fa-check"></i><b>6.1</b> A cautionary detour</a></li>
<li class="chapter" data-level="6.2" data-path="6-simple-strategy.html"><a href="6-simple-strategy.html#delta-method"><i class="fa fa-check"></i><b>6.2</b> ⚙ Delta-method</a></li>
<li class="chapter" data-level="6.3" data-path="6-simple-strategy.html"><a href="6-simple-strategy.html#known-gbar-first-pass"><i class="fa fa-check"></i><b>6.3</b> IPTW estimator assuming the mechanism of action known</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-simple-strategy.html"><a href="6-simple-strategy.html#a-simple-estimator"><i class="fa fa-check"></i><b>6.3.1</b> A simple estimator</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-simple-strategy.html"><a href="6-simple-strategy.html#elementary-statistical-properties"><i class="fa fa-check"></i><b>6.3.2</b> Elementary statistical properties</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-simple-strategy.html"><a href="6-simple-strategy.html#empirical-inves-IPTW"><i class="fa fa-check"></i><b>6.3.3</b> Empirical investigation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-nuisance.html"><a href="7-nuisance.html"><i class="fa fa-check"></i><b>7</b> Nuisance parameters</a><ul>
<li class="chapter" data-level="7.1" data-path="7-nuisance.html"><a href="7-nuisance.html#anatomy"><i class="fa fa-check"></i><b>7.1</b> Anatomy of an expression</a></li>
<li class="chapter" data-level="7.2" data-path="7-nuisance.html"><a href="7-nuisance.html#an-algorithmic-stance"><i class="fa fa-check"></i><b>7.2</b> An algorithmic stance</a></li>
<li class="chapter" data-level="7.3" data-path="7-nuisance.html"><a href="7-nuisance.html#nuisance-QW"><i class="fa fa-check"></i><b>7.3</b> <code>QW</code></a></li>
<li class="chapter" data-level="7.4" data-path="7-nuisance.html"><a href="7-nuisance.html#nuisance-Gbar"><i class="fa fa-check"></i><b>7.4</b> <code>Gbar</code></a><ul>
<li class="chapter" data-level="7.4.1" data-path="7-nuisance.html"><a href="7-nuisance.html#logis-loss"><i class="fa fa-check"></i><b>7.4.1</b> Working model-based algorithms</a></li>
<li class="chapter" data-level="7.4.2" data-path="7-nuisance.html"><a href="7-nuisance.html#algo-Gbar-one"><i class="fa fa-check"></i><b>7.4.2</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7-nuisance.html"><a href="7-nuisance.html#nuisance-Qbar-wm"><i class="fa fa-check"></i><b>7.5</b> ⚙ <code>Qbar</code>, working model-based algorithms</a></li>
<li class="chapter" data-level="7.6" data-path="7-nuisance.html"><a href="7-nuisance.html#nuisance-Qbar"><i class="fa fa-check"></i><b>7.6</b> <code>Qbar</code></a><ul>
<li class="chapter" data-level="7.6.1" data-path="7-nuisance.html"><a href="7-nuisance.html#qbar-machine-learning-based-algorithms"><i class="fa fa-check"></i><b>7.6.1</b> <code>Qbar</code>, machine learning-based algorithms</a></li>
<li class="chapter" data-level="7.6.2" data-path="7-nuisance.html"><a href="7-nuisance.html#Qbar-knn-algo"><i class="fa fa-check"></i><b>7.6.2</b> <code>Qbar</code>, kNN algorithm</a></li>
<li class="chapter" data-level="7.6.3" data-path="7-nuisance.html"><a href="7-nuisance.html#boosted-trees"><i class="fa fa-check"></i><b>7.6.3</b> <code>Qbar</code>, boosted trees algorithm</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="7-nuisance.html"><a href="7-nuisance.html#nuisance-Qbar-ml-exo"><i class="fa fa-check"></i><b>7.7</b> ⚙ ☡  <code>Qbar</code>, machine learning-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html"><i class="fa fa-check"></i><b>8</b> Two “naive” inference strategies</a><ul>
<li class="chapter" data-level="8.1" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#why-naive"><i class="fa fa-check"></i><b>8.1</b> Why “naive”?</a></li>
<li class="chapter" data-level="8.2" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#known-gbar-second-pass"><i class="fa fa-check"></i><b>8.2</b> IPTW estimator</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#unknown-gbar-constr"><i class="fa fa-check"></i><b>8.2.1</b> Construction and computation</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#elementary-stat-prop-iptw"><i class="fa fa-check"></i><b>8.2.2</b> Elementary statistical properties</a></li>
<li class="chapter" data-level="8.2.3" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#empirical-inves-IPTW-bis"><i class="fa fa-check"></i><b>8.2.3</b> Empirical investigation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#exo-a-nice-title"><i class="fa fa-check"></i><b>8.3</b> ⚙ Investigating further the IPTW inference strategy</a></li>
<li class="chapter" data-level="8.4" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#Gcomp-estimator"><i class="fa fa-check"></i><b>8.4</b> G-computation estimator</a><ul>
<li class="chapter" data-level="8.4.1" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#Gcomp-construction"><i class="fa fa-check"></i><b>8.4.1</b> Construction and computation</a></li>
<li class="chapter" data-level="8.4.2" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#elementary-statistical-properties-1"><i class="fa fa-check"></i><b>8.4.2</b> Elementary statistical properties</a></li>
<li class="chapter" data-level="8.4.3" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#empirical-inves-Gcomp"><i class="fa fa-check"></i><b>8.4.3</b> Empirical investigation, fixed sample size</a></li>
<li class="chapter" data-level="8.4.4" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#empirical-inves-Gcomp-varying"><i class="fa fa-check"></i><b>8.4.4</b> ☡  Empirical investigation, varying sample size</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8-naive-estimators.html"><a href="8-naive-estimators.html#exo-plug-in-estimate"><i class="fa fa-check"></i><b>8.5</b> ⚙ Investigating further the G-computation estimation strategy</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-one-step.html"><a href="9-one-step.html"><i class="fa fa-check"></i><b>9</b> One-step correction</a><ul>
<li class="chapter" data-level="9.1" data-path="9-one-step.html"><a href="9-one-step.html#analysis-of-plug-in"><i class="fa fa-check"></i><b>9.1</b> ☡  General analysis of plug-in estimators</a></li>
<li class="chapter" data-level="9.2" data-path="9-one-step.html"><a href="9-one-step.html#huber-one-step"><i class="fa fa-check"></i><b>9.2</b> One-step correction</a></li>
<li class="chapter" data-level="9.3" data-path="9-one-step.html"><a href="9-one-step.html#empirical-inves-one-step"><i class="fa fa-check"></i><b>9.3</b> Empirical investigation</a></li>
<li class="chapter" data-level="9.4" data-path="9-one-step.html"><a href="9-one-step.html#exo-one-step"><i class="fa fa-check"></i><b>9.4</b> ⚙ Investigating further the one-step correction methodology</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-TMLE.html"><a href="10-TMLE.html"><i class="fa fa-check"></i><b>10</b> Targeted minimum loss-based estimation</a><ul>
<li class="chapter" data-level="10.1" data-path="10-TMLE.html"><a href="10-TMLE.html#TMLE-motivations"><i class="fa fa-check"></i><b>10.1</b> Motivations</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-TMLE.html"><a href="10-TMLE.html#falling-outside-the-parameter-space"><i class="fa fa-check"></i><b>10.1.1</b> Falling outside the parameter space</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-TMLE.html"><a href="10-TMLE.html#eic-equation"><i class="fa fa-check"></i><b>10.1.2</b> The influence curve equation</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-TMLE.html"><a href="10-TMLE.html#basic-fact"><i class="fa fa-check"></i><b>10.1.3</b> A basic fact on the influence curve equation</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-TMLE.html"><a href="10-TMLE.html#targeted-fluctuation-TMLE"><i class="fa fa-check"></i><b>10.2</b> Targeted fluctuation</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-TMLE.html"><a href="10-TMLE.html#fluctuating-indirectly"><i class="fa fa-check"></i><b>10.2.1</b> ☡  Fluctuating indirectly</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-TMLE.html"><a href="10-TMLE.html#fluct-direct"><i class="fa fa-check"></i><b>10.2.2</b> Fluctuating directly</a></li>
<li class="chapter" data-level="10.2.3" data-path="10-TMLE.html"><a href="10-TMLE.html#exo-fluct"><i class="fa fa-check"></i><b>10.2.3</b> ⚙ More on fluctuations</a></li>
<li class="chapter" data-level="10.2.4" data-path="10-TMLE.html"><a href="10-TMLE.html#roaming"><i class="fa fa-check"></i><b>10.2.4</b> Targeted roaming of a fluctuation</a></li>
<li class="chapter" data-level="10.2.5" data-path="10-TMLE.html"><a href="10-TMLE.html#fluct-justification"><i class="fa fa-check"></i><b>10.2.5</b> Justifying the form of the fluctutation</a></li>
<li class="chapter" data-level="10.2.6" data-path="10-TMLE.html"><a href="10-TMLE.html#exo-tmle-flucs"><i class="fa fa-check"></i><b>10.2.6</b> ⚙ Alternative fluctuation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-TMLE.html"><a href="10-TMLE.html#summary-and-perspectives"><i class="fa fa-check"></i><b>10.3</b> Summary and perspectives</a></li>
<li class="chapter" data-level="10.4" data-path="10-TMLE.html"><a href="10-TMLE.html#empirical-inves-tmle"><i class="fa fa-check"></i><b>10.4</b> Empirical investigation</a><ul>
<li class="chapter" data-level="10.4.1" data-path="10-TMLE.html"><a href="10-TMLE.html#empirical-inves-tmle-first"><i class="fa fa-check"></i><b>10.4.1</b> A first numerical application</a></li>
<li class="chapter" data-level="10.4.2" data-path="10-TMLE.html"><a href="10-TMLE.html#exo-tmle"><i class="fa fa-check"></i><b>10.4.2</b> ⚙ A computational exploration</a></li>
<li class="chapter" data-level="10.4.3" data-path="10-TMLE.html"><a href="10-TMLE.html#empirical-investigation"><i class="fa fa-check"></i><b>10.4.3</b> Empirical investigation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-closing-words.html"><a href="11-closing-words.html"><i class="fa fa-check"></i><b>11</b> Closing words</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-notation.html"><a href="A-notation.html"><i class="fa fa-check"></i><b>A</b> Notation</a></li>
<li class="chapter" data-level="B" data-path="B-proofs.html"><a href="B-proofs.html"><i class="fa fa-check"></i><b>B</b> Basic results and their proofs</a><ul>
<li class="chapter" data-level="B.1" data-path="B-proofs.html"><a href="B-proofs.html#npsem"><i class="fa fa-check"></i><b>B.1</b> NPSEM</a></li>
<li class="chapter" data-level="B.2" data-path="B-proofs.html"><a href="B-proofs.html#identification"><i class="fa fa-check"></i><b>B.2</b> Identification</a></li>
<li class="chapter" data-level="B.3" data-path="B-proofs.html"><a href="B-proofs.html#confidence-interval"><i class="fa fa-check"></i><b>B.3</b> Building a confidence interval</a><ul>
<li class="chapter" data-level="B.3.1" data-path="B-proofs.html"><a href="B-proofs.html#clt"><i class="fa fa-check"></i><b>B.3.1</b> CLT &amp; Slutsky’s lemma</a></li>
<li class="chapter" data-level="B.3.2" data-path="B-proofs.html"><a href="B-proofs.html#order"><i class="fa fa-check"></i><b>B.3.2</b> CLT and order statistics</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="B-proofs.html"><a href="B-proofs.html#another-rep"><i class="fa fa-check"></i><b>B.4</b> Another representation of the parameter of interest</a></li>
<li class="chapter" data-level="B.5" data-path="B-proofs.html"><a href="B-proofs.html#prop-delta-method"><i class="fa fa-check"></i><b>B.5</b> The delta-method</a></li>
<li class="chapter" data-level="B.6" data-path="B-proofs.html"><a href="B-proofs.html#oracle-logistic-risk"><i class="fa fa-check"></i><b>B.6</b> The oracle logistic risk</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-more-proofs.html"><a href="C-more-proofs.html"><i class="fa fa-check"></i><b>C</b> More results and their proofs</a><ul>
<li class="chapter" data-level="C.1" data-path="C-more-proofs.html"><a href="C-more-proofs.html#estimation-of-the-asymptotic-variance-of-an-estimator"><i class="fa fa-check"></i><b>C.1</b> Estimation of the asymptotic variance of an estimator</a><ul>
<li class="chapter" data-level="C.1.1" data-path="C-more-proofs.html"><a href="C-more-proofs.html#iptw-est-var"><i class="fa fa-check"></i><b>C.1.1</b> IPTW estimator based on a well-specified model</a></li>
<li class="chapter" data-level="C.1.2" data-path="C-more-proofs.html"><a href="C-more-proofs.html#gcomp-est-var"><i class="fa fa-check"></i><b>C.1.2</b> G-computation estimator based on a well-specified model</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="C-more-proofs.html"><a href="C-more-proofs.html#app-analysis-of-plug-in"><i class="fa fa-check"></i><b>C.2</b> ☡  General analysis of plug-in estimators</a><ul>
<li class="chapter" data-level="C.2.1" data-path="C-more-proofs.html"><a href="C-more-proofs.html#app-analysis-of-plug-in-main"><i class="fa fa-check"></i><b>C.2.1</b> Main analysis</a></li>
<li class="chapter" data-level="C.2.2" data-path="C-more-proofs.html"><a href="C-more-proofs.html#estimation-of-the-asymptotic-variance"><i class="fa fa-check"></i><b>C.2.2</b> Estimation of the asymptotic variance</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="C-more-proofs.html"><a href="C-more-proofs.html#asymp-neglig-remain"><i class="fa fa-check"></i><b>C.3</b> Asymptotic negligibility of the remainder term</a></li>
<li class="chapter" data-level="C.4" data-path="C-more-proofs.html"><a href="C-more-proofs.html#analysis-TMLE"><i class="fa fa-check"></i><b>C.4</b> Analysis of targeted estimators</a><ul>
<li class="chapter" data-level="C.4.1" data-path="C-more-proofs.html"><a href="C-more-proofs.html#basic-eic-eq"><i class="fa fa-check"></i><b>C.4.1</b> A basic fact on the influence curve equation</a></li>
<li class="chapter" data-level="C.4.2" data-path="C-more-proofs.html"><a href="C-more-proofs.html#fluct-reg"><i class="fa fa-check"></i><b>C.4.2</b> Fluctuation of the regression function along the fluctuation of a law</a></li>
<li class="chapter" data-level="C.4.3" data-path="C-more-proofs.html"><a href="C-more-proofs.html#fluct-score"><i class="fa fa-check"></i><b>C.4.3</b> Computing the score of a fluctuation of the regression function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="D-references.html"><a href="D-references.html"><i class="fa fa-check"></i><b>D</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Ride in Targeted Learning Territory</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(\newcommand{\bbO}{\mathbb{O}}\)
\(\newcommand{\bbD}{\mathbb{D}}\)
\(\newcommand{\bbP}{\mathbb{P}}\)
\(\newcommand{\bbR}{\mathbb{R}}\)
\(\newcommand{\Algo}{\widehat{\mathcal{A}}}\)
\(\newcommand{\Algora}{\widetilde{\mathcal{A}}}\)
\(\newcommand{\calF}{\mathcal{F}}\)
\(\newcommand{\calM}{\mathcal{M}}\)
\(\newcommand{\calP}{\mathcal{P}}\)
\(\newcommand{\calO}{\mathcal{O}}\)
\(\newcommand{\calQ}{\mathcal{Q}}\)
\(\newcommand{\defq}{\doteq}\)
\(\newcommand{\Exp}{\textrm{E}}\)
\(\newcommand{\IC}{\textrm{IC}}\)
\(\newcommand{\Gbar}{\bar{G}}\)
\(\newcommand{\one}{\textbf{1}}\)
\(\newcommand{\psinos}{\psi_{n}^{\textrm{os}}}\)
\(\renewcommand{\Pr}{\textrm{Pr}}\)
\(\newcommand{\Phat}{P^{\circ}}\)
\(\newcommand{\Psihat}{\widehat{\Psi}}\)
\(\newcommand{\Qbar}{\bar{Q}}\)
\(\newcommand{\tcg}[1]{\textcolor{olive}{#1}}\)
\(\DeclareMathOperator{\Dirac}{Dirac}\)
\(\DeclareMathOperator{\expit}{expit}\)
\(\DeclareMathOperator{\logit}{logit}\)
\(\DeclareMathOperator{\Rem}{Rem}\)
\(\DeclareMathOperator{\Var}{Var}\)
<div id="more-proofs" class="section level1">
<h1><span class="header-section-number">C</span> More results and their proofs</h1>
<div id="estimation-of-the-asymptotic-variance-of-an-estimator" class="section level2">
<h2><span class="header-section-number">C.1</span> Estimation of the asymptotic variance of an estimator</h2>
<div id="iptw-est-var" class="section level3">
<h3><span class="header-section-number">C.1.1</span> IPTW estimator based on a well-specified model</h3>
<p><strong>Sketch of proof.</strong> The IPTW estimator <span class="math inline">\(\psi_{n}^{b}\)</span> relies on algorithm
<span class="math inline">\(\Algo_{\Gbar,1}\)</span>, which is “well-specified” in the
sense that its output <span class="math inline">\(\Gbar_{n}\defq \Algo_{\Gbar,1}(P_{n})\)</span> minimizes the
empirical risk over a finite-dimensional, identifiable, well-specified working
model for <span class="math inline">\(\Gbar_{0}\)</span>. If one introduces <span class="math inline">\(D\)</span> given by</p>
<p><span class="math display">\[\begin{equation*}
D(O) \defq \frac{(2A-1)}{\ell\Gbar_{0}(A,W)} Y,
\end{equation*}\]</span></p>
<p>then the influence curve of <span class="math inline">\(\psi_{n}^{b}\)</span> equals <span class="math inline">\(D - \Psi(P_{0})\)</span> minus the
projection of <span class="math inline">\(D\)</span> onto the tangent space of the above parametric model for
<span class="math inline">\(\Gbar_{0}\)</span>. The variance of the influence curve is thus smaller than that of
<span class="math inline">\(D\)</span>, hence the conservativeness.</p>
</div>
<div id="gcomp-est-var" class="section level3">
<h3><span class="header-section-number">C.1.2</span> G-computation estimator based on a well-specified model</h3>
<p><strong>Sketch of proof</strong> (see <span class="citation">(Laan and Rose 2011)</span> page 527). Consider a G-computation
estimator <span class="math inline">\(\psi_{n}\)</span> that relies on an algorithm <span class="math inline">\(\Algo_{\Qbar}\)</span> that is
“well-specified” in the sense that its output
<span class="math inline">\(\Qbar_{n}\defq \Algo_{\Qbar}(P_{n})\)</span> minimizes the empirical risk over a
finite-dimensional, identifiable, well-specified working model for
<span class="math inline">\(\Qbar_{0}\)</span>. If one introduces <span class="math inline">\(D\)</span> given by</p>
<p><span class="math display">\[\begin{equation*}
D(O) \defq \Qbar_{0}(1,W) - \Qbar_{0}(0,W)
\end{equation*}\]</span></p>
<p>then the influence curve of <span class="math inline">\(\psi_{n}\)</span> equals <span class="math inline">\(D - \Psi(P_{0})\)</span> plus a
function of <span class="math inline">\(O\)</span> that is orthogonal to <span class="math inline">\(D - \Psi(P_{0})\)</span>. Thus the variance of
the influence curve is larger than that of <span class="math inline">\(D\)</span>, hence the
anti-conservativeness.</p>
</div>
</div>
<div id="app-analysis-of-plug-in" class="section level2">
<h2><span class="header-section-number">C.2</span> ☡  General analysis of plug-in estimators</h2>
<p>Recall that <span class="math inline">\(\Algo_{Q_{W}}\)</span> is an algorithm designed for the estimation of
<span class="math inline">\(Q_{0,W}\)</span> (see Section <a href="7-nuisance.html#nuisance-QW">7.3</a>) and that we denote by <span class="math inline">\(Q_{n,W} \triangleq \Algo_{Q_{W}}(P_{n})\)</span> the output of the algorithm trained on <span class="math inline">\(P_{n}\)</span>.
Likewise, <span class="math inline">\(\Algo_{\Gbar}\)</span> and <span class="math inline">\(\Algo_{\Qbar}\)</span> are two generic algorithms
designed for the estimation of <span class="math inline">\(\Gbar_{0}\)</span> and of <span class="math inline">\(\Qbar_{0}\)</span> (see Sections
<a href="7-nuisance.html#nuisance-Gbar">7.4</a> and <a href="7-nuisance.html#nuisance-Qbar">7.6</a>), <span class="math inline">\(\Gbar_{n} \triangleq \Algo_{\Gbar}(P_{n})\)</span> and <span class="math inline">\(\Qbar_{n} \triangleq \Algo_{\Qbar}(P_{n})\)</span> are their
outputs once trained on <span class="math inline">\(P_{n}\)</span>.</p>
<p>Let us now introduce <span class="math inline">\(\Phat_n\)</span> a law in <span class="math inline">\(\calM\)</span> such that the <span class="math inline">\(Q_{W}\)</span>, <span class="math inline">\(\Gbar\)</span>
and <span class="math inline">\(\Qbar\)</span> features of <span class="math inline">\(\Phat_n\)</span> equal <span class="math inline">\(Q_{n,W}\)</span>, <span class="math inline">\(\Gbar_{n}\)</span> and
<span class="math inline">\(\Qbar_{n}\)</span>, respectively. We say that any such law is <em>compatible</em> with
<span class="math inline">\(Q_{n,W}\)</span>, <span class="math inline">\(\Gbar_n\)</span> and <span class="math inline">\(\Qbar_n\)</span>.</p>
<div id="app-analysis-of-plug-in-main" class="section level3">
<h3><span class="header-section-number">C.2.1</span> Main analysis</h3>
<p>Substituting <span class="math inline">\(\Phat_n\)</span> for <span class="math inline">\(P\)</span> in <a href="4-double-robustness.html#eq:taylor-expansion">(4.1)</a> yields
<a href="9-one-step.html#eq:hard-to-study">(9.1)</a>:
<span class="math display">\[\begin{equation} 
\sqrt{n} (\Psi(\Phat_n) - \Psi(P_0)) =  - \sqrt{n} P_0 D^*(\Phat_n) + \sqrt{n}
\Rem_{P_0}(\Phat_n), 
\end{equation}\]</span></p>
<p>an equality that we rewrite as
<span class="math display">\[\begin{align} 
\sqrt{n} (\Psi(\Phat_n) - \Psi(P_0)) = - &amp; \sqrt{n} P_n D^*(\Phat_n) + \sqrt{n}
(P_n - P_0) D^*(P_0)\\ &amp; + \sqrt{n}(P_n - P_0) [D^*(\Phat_n) - D^*(P_0)] +
\sqrt{n}\Rem_{P_0}(\Phat_n). 
\end{align}\]</span></p>
<p>Let us know study in turn the four terms in the above right-hand side
sum. Recall that <span class="math inline">\(X_n = o_{P_0}(1)\)</span> means that <span class="math inline">\(P_0(|X_n| &gt; t)\)</span> converges to
zero for all <span class="math inline">\(t&gt;0\)</span> as <span class="math inline">\(n\)</span> goes to infinity.</p>
<ol style="list-style-type: decimal">
<li><p>In view of <a href="4-double-robustness.html#eq:rem-two">(4.4)</a>, the fourth term is <span class="math inline">\(o_{P_0}(1)\)</span> provided that
<span class="math inline">\(\sqrt{n}\|\Qbar - \Qbar_{0}\|_{P_0} \times \|(\Gbar - \Gbar_{0})/\ell\Gbar_{0}\|_{P_0} = o_{P_0}(1)\)</span>. This is the case if, for
instance, <span class="math inline">\(\ell\Gbar_{0}\)</span> is bounded away from zero, and both
<span class="math inline">\(n^{1/4}\|\Qbar - \Qbar_{0}\|_{P_0}\)</span> and <span class="math inline">\(n^{1/4}\|\Gbar - \Gbar_{0}\|_{P_0}\)</span> are <span class="math inline">\(o_{P_0}(1)\)</span>. What really matters, remarkably, is
the <em>product</em> of the two norms. If each norm goes to zero at rate
<span class="math inline">\(n^{1/4}\)</span>, then their product does at rate <span class="math inline">\(\sqrt{n}\)</span>. Of course, if one
goes to zero at rate <span class="math inline">\(n^{1/4 + c}\)</span> for some <span class="math inline">\(0&lt;c&lt;1/4\)</span>, then it suffices
that the other go to zero at rate <span class="math inline">\(n^{1/4 - c}\)</span>. See also Section
<a href="C-more-proofs.html#asymp-neglig-remain">C.3</a>.</p></li>
<li><p>A fundamental result from empirical processes theory gives us conditions
guaranteeing that the third term is <span class="math inline">\(o_{P_0}(1)\)</span>. By Lemma 19.24 in
<span class="citation">(Vaart 1998)</span>, this is the case indeed if <span class="math inline">\(\|D^*(\Phat_n) - D^*(P_0)\|_{P_{0}} = o_{P_0} (1)\)</span> (that is, if <span class="math inline">\(D^*(\Phat_n)\)</span> estimates consistently <span class="math inline">\(D^*(P_0)\)</span>) and if
<span class="math inline">\(D^*(\Phat_n)\)</span> falls (with probability tending to one) into a Donsker class
(meaning that the random <span class="math inline">\(D^*(\Phat_n)\)</span> must belong eventually to a set
that is not too large). Requesting that <span class="math inline">\(\|D^*(\Phat_n) - D^*(P_0)\|_{P_{0}} = o_{P_0} (1)\)</span> is not much if one is already willing to assume that
<span class="math inline">\(n^{1/4}\|\Qbar - \Qbar_{0}\|_{P_0}\)</span> and <span class="math inline">\(n^{1/4}\|\Gbar - \Gbar_{0}\|_{P_0}\)</span> are <span class="math inline">\(o_{P_0}(1)\)</span>. Moreover, the second condition can be
interpreted as a condition on the complexity/versatility of algorithms
<span class="math inline">\(\Algo_{\Gbar}\)</span> and <span class="math inline">\(\Algo_{\Qbar}\)</span>.</p></li>
<li><p>By the central limit theorem, the second term converges in law to the
centered Gaussian law with variance <span class="math inline">\(P_0 D^*(P_0)^2\)</span>.</p></li>
<li><p>As for the first term, all we can say is that it is a potentially large
(because of the <span class="math inline">\(\sqrt{n}\)</span> renormalization factor) <em>bias term</em>.</p></li>
</ol>
</div>
<div id="estimation-of-the-asymptotic-variance" class="section level3">
<h3><span class="header-section-number">C.2.2</span> Estimation of the asymptotic variance</h3>
<p>Let us show now that, under the assumptions we made in Section
<a href="C-more-proofs.html#app-analysis-of-plug-in-main">C.2.1</a> and additional assumptions of similar
nature, <span class="math inline">\(P_{n} D^{*} (\Phat_n)^2\)</span> estimates consistently the asymptotic
variance <span class="math inline">\(P_{0} D^{*} (P_{0})^2\)</span>. The proof hinges again on a decomposition
of the difference between the two quantities as a sum of three terms:
<span class="math display">\[\begin{align}  P_{n} D^{*}  (\Phat_n)^2 -  P_{0} D^{*}  (P_{0})^2= &amp;  (P_{n} -
P_{0}) \left(D^{*} (\Phat_n)^2 - D^{*}  (P_{0})^2\right)\\ &amp; + (P_{n} - P_{0})
D^{*}  (P_{0})^2 +  P_{0}  (D^{*} \left(\Phat_n)^2  - D^{*}  (P_{0})^2\right).
\end{align}\]</span></p>
<p>We study the three terms in turn. Recall that <span class="math inline">\(X_n = o_{P_0}(1/\sqrt{n})\)</span>
means that <span class="math inline">\(P_0(\sqrt{n}|X_n| &gt; t)\)</span> converges to zero for all <span class="math inline">\(t&gt;0\)</span> as <span class="math inline">\(n\)</span>
goes to infinity.</p>
<ol style="list-style-type: decimal">
<li><p>In light of the study of the third term in Section
<a href="C-more-proofs.html#app-analysis-of-plug-in-main">C.2.1</a>, if <span class="math inline">\(\|D^*(\Phat_n)^2 - D^*(P_0)^2\|_{P_{0}} = o_{P_0} (1)\)</span> and if <span class="math inline">\(D^*(\Phat_n)^{2}\)</span> falls (with
probability tending to one) into a Donsker class, then the first term is
<span class="math inline">\(o_{P_0}(1/\sqrt{n})\)</span>. Furthermore, if <span class="math inline">\(D^*(\Phat_n)\)</span> falls (with
probability tending to one) into a Donsker class, an assumption we made
earlier, then so does <span class="math inline">\(D^*(\Phat_n)^{2}\)</span>. In addition, if
<span class="math inline">\(\|D^*(\Phat_n) - D^*(P_0)\|_{P_{0}} = o_{P_0} (1)\)</span>, another assumption we
made earlier, and if there exists a constant <span class="math inline">\(c&gt;0\)</span> such that
<span class="math display">\[\begin{equation} \sup_{n \geq 1}  \|D^*(\Phat_n) + D^*(P_0)\|_{\infty} \leq
c \tag{C.1}  \end{equation}\]</span> <span class="math inline">\(P_{0}\)</span>-almost surely, then <span class="math inline">\(\|D^*(\Phat_n)^2 - D^*(P_0)^2\|_{P_{0}} = o_{P_0} (1)\)</span> too because <span class="math display" id="eq:bounded-sup-norm">\[\begin{equation}
\|D^*(\Phat_n)^2   -   D^*(P_0)^2\|_{P_{0}}   \leq   c   \|D^*(\Phat_n)   -
D^*(P_0)\|_{P_{0}}.  \end{equation}\]</span> The existence of such a constant <span class="math inline">\(c\)</span> is
granted whenever <span class="math inline">\(\ell\Gbar_{0}\)</span> and <span class="math inline">\(\ell\Gbar_{n}\)</span> are bounded away from
zero. Note that the condition on <span class="math inline">\(\ell\Gbar_{n}\)</span> can be inforced by us
through the specification of algorithm <span class="math inline">\(\Algo_{\Gbar}\)</span>.</p></li>
<li><p>By the central limit theorem, <span class="math inline">\(\sqrt{n}\)</span> times the second term converges in
law to the centered Gaussian law with variance <span class="math inline">\(\Var_{P_{0}} (D^{*} (P_{0})(O)^2)\)</span>, which is finite whenever <span class="math inline">\(\ell\Gbar_{0}\)</span> is bounded away
from zero. By Theorem 2.4 in <span class="citation">(Vaart 1998)</span>, the second term is thus <span class="math inline">\(O_{P_0} (1/\sqrt{n})\)</span> hence <span class="math inline">\(o_{P_0} (1)\)</span>.</p></li>
<li><p>Finally, under assumption <a href="C-more-proofs.html#eq:bounded-sup-norm">(C.1)</a>, the absolute value of
the third term is smaller than <span class="math display">\[\begin{equation}c  P_{0} |D^*(\Phat_n)  -
D^*(P_0)|     \leq    c     \|D^*(\Phat_n)    -     D^*(P_0)\|_{P_{0}}    =
o_{P_0}(1),\end{equation}\]</span> where the inequality follows from the
Cauchy-Schwarz inequality.</p></li>
</ol>
<p>In conclusion, <span class="math inline">\(P_{n} D^{*} (\Phat_n)^2 - P_{0} D^{*} (P_{0})^2 = o_{P_0}(1)\)</span>,
hence the result.</p>
</div>
</div>
<div id="asymp-neglig-remain" class="section level2">
<h2><span class="header-section-number">C.3</span> Asymptotic negligibility of the remainder term</h2>
<p>Recall that <span class="math inline">\(\|f\|_{P}^{2} \defq \Exp_{P} \left( f(O)^{2} \right)\)</span> is the
<span class="math inline">\(L_2(P)\)</span>-norm of <span class="math inline">\(f\)</span>, a measurable function from <span class="math inline">\(\calO\)</span> to <span class="math inline">\(\bbR\)</span>. Assume
that for <span class="math inline">\(a= 0,1\)</span>, <span class="math inline">\(\ell\Gbar_{n}(a,W) \geq \delta &gt; 0\)</span> <span class="math inline">\(Q_{0,W}\)</span>-almost
everywhere.</p>
<p>The Cauchy-Schwarz inequality then implies that, for <span class="math inline">\(a = 0,1\)</span>,
<span class="math display">\[\begin{equation*}\Rem_{P_0}(\Phat_n)   \le   \frac{2}{\delta}   \max_{a=0,1}
\left(  \|\Qbar_n   (a,\cdot)  -  \Qbar_0  (a,\cdot)\|_{P_0}   \right)  \times
\|\Gbar_n  -   \Gbar_0\|_{P_0}.\end{equation*}\]</span> Therefore, if for <span class="math inline">\(a=0,1\)</span>,
<span class="math display">\[\begin{equation*}\|\Qbar_n(a,\cdot)      -     \Qbar_0(a,\cdot)\|_{P_0}      =
o_{P_0}(n^{-1/4})\end{equation*}\]</span> <em>and</em> <span class="math display">\[\begin{equation*}\|\Gbar_n     -
\Gbar_0\|_{P_0}        =         o_{P_0}(n^{-1/4}),\end{equation*}\]</span> then
<span class="math display">\[\begin{equation*}\Rem_{P_0}(\Phat_n) = o_{P_0}(n^{-1/2}).\end{equation*}\]</span></p>
</div>
<div id="analysis-TMLE" class="section level2">
<h2><span class="header-section-number">C.4</span> Analysis of targeted estimators</h2>
<div id="basic-eic-eq" class="section level3">
<h3><span class="header-section-number">C.4.1</span> A basic fact on the influence curve equation</h3>
<p>Recall the definition of <span class="math inline">\(D_{1}^{*}\)</span> <a href="3-smooth.html#eq:eif">(3.4)</a>. For <em>any</em> estimator
<span class="math inline">\(\Qbar_n^*\)</span> of <span class="math inline">\(\Qbar_0\)</span> and a law <span class="math inline">\(P_n^{*}\)</span> that is compatible with
<span class="math inline">\(\Qbar_n^*\)</span> and <span class="math inline">\(Q_{n,W}\)</span>, it holds that
<span class="math display">\[\begin{align*}
P_n D_1^*(P_n^*) 
&amp;= \frac{1}{n} \sum_{i=1}^n D_1^{*}(P_n^*)(O_i)\\ 
&amp;=\frac{1}{n}  \sum_{i=1}^n  \left(  \Qbar_n(1,W_i)   -  \Qbar_n(0,W_i)  -  \int
\left(\Qbar_n(1,w) - \Qbar_n(0,w)\right) dQ_{n,W}(w) \right) \\ 
&amp;=   \frac{1}{n}\sum_{i=1}^n \left( \Qbar_n(1,W_i)  - \Qbar_n(0,W_i)\right) - \frac{1}{n}\sum_{i=1}^n
\left(\Qbar_n(1,W_i) - \Qbar_n(0,W_i)\right) = 0.
\end{align*}\]</span></p>
</div>
<div id="fluct-reg" class="section level3">
<h3><span class="header-section-number">C.4.2</span> Fluctuation of the regression function along the fluctuation of a law</h3>
<p>Let us resume the discussion where we left it at the end of Section
<a href="3-smooth.html#smooth-second-pass-fluctuations">3.3.1</a>. Let <span class="math inline">\(\Qbar\)</span> be the conditional mean
of <span class="math inline">\(Y\)</span> given <span class="math inline">\((A,W)\)</span> under <span class="math inline">\(P\)</span>. Set arbitrarily <span class="math inline">\(h \in H \setminus \{0\}\)</span> and
a measurable function <span class="math inline">\((w,a) \mapsto f(a,w)\)</span> taking non-negative
values. Applying repeatedly the tower rule yields the following equalities:
<span class="math display">\[\begin{align*} \Exp_{P_{h}}  \left(f(A,W) Y\right) &amp;= \Exp_{P}  \left(f(A,W) Y
(1 + h  s(O))\right) \\ &amp;= \Exp_{P} \left(f(A,W) \Exp_{P}\left(Y  (1 + h s(O))
\middle|A,W\right)\right)  \\ &amp;=  \Exp_{P} \left(f(A,W)  \left(\Qbar(A,W) +  h
\Exp_{P}(Ys(O)   |   A,W)   \right)\right)   \\   &amp;=   \Exp_{P}   \left(f(A,W)
\frac{\Qbar(A,W) + h  \Exp_{P}(Ys(O) | A,W)}{1 +  h \Exp_{P}(s(O)|A,W)} \times
\left(1 + h \Exp_{P}(s(O)|A,W)\right)\right).\end{align*}\]</span> Now, <a href="3-smooth.html#eq:fluct">(3.1)</a>
implies that the density of <span class="math inline">\((A,W)\)</span> under <span class="math inline">\(P_{h}\)</span> equals <span class="math inline">\(\left(1 + h \Exp_{P}(s(O)|A,W)\right)\)</span> when it is evaluated at <span class="math inline">\((A,W)\)</span>. Therefore, the
last inequality rewrites as
<span class="math display">\[\begin{equation*} 
\Exp_{P_{h}}  \left(f(A,W)
Y\right)  = \Exp_{P_{h}}  \left(f(A,W) \frac{\Qbar(A,W)  + h  \Exp_{P}(Ys(O) |
A,W)}{1 + h \Exp_{P}(s(O)|A,W)}\right).
\end{equation*}\]</span>
Since this equality is valid
for an arbitary <span class="math inline">\((w,a) \mapsto f(a,w)\)</span> with non-negative values, we can deduce
from it that the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\((A,W)\)</span> under <span class="math inline">\(P_h\)</span> equals
<span class="math display">\[\begin{equation*}\frac{\Qbar(A,W)   +  h   \Exp_{P}(Ys(O)   |   A,W)}{1  +   h
\Exp_{P}(s(O)|A,W)}.\end{equation*}\]</span></p>
</div>
<div id="fluct-score" class="section level3">
<h3><span class="header-section-number">C.4.3</span> Computing the score of a fluctuation of the regression function</h3>
<p>Let us resume the discussion where we left it at the beginning of Section
<a href="10-TMLE.html#fluct-direct">10.2.2</a>. Set <span class="math inline">\(\alpha, \beta \in \bbR\)</span>. The derivative of <span class="math inline">\(h \mapsto \expit(\alpha + \beta h)\)</span> evaluated at <span class="math inline">\(h=0\)</span> satisfies
<span class="math display">\[\begin{equation*}\frac{d}{dh}  \left.\expit(\alpha +  \beta h)\right|_{h=0}  =
\beta \expit(\alpha)  (1 - \expit(\alpha)).\end{equation*}\]</span> Therefore, for any
<span class="math inline">\((w,a) \in [0,1] \times \{0,1\}\)</span>,
<span class="math display">\[\begin{align*}\frac{d}{dh} \left.\Qbar_{h} (a, w)\right|_{h=0} &amp;= \frac{2a - 1}{\ell\Gbar(a,w)} \expit\left(\logit\left(\Qbar(a,w)\right)\right) \left[1 - \expit\left(\logit\left(\Qbar(a,w)\right)\right)\right]\\&amp;= \frac{2a - 1}{\ell\Gbar(a,w)} \Qbar(a,w) \left(1 - \Qbar(a,w)\right).\end{align*}\]</span>
This justifies the last but one equality in <a href="10-TMLE.html#eq:fluct-score">(10.8)</a>.</p>
<p>Furthermore the same derivations that led to <a href="10-TMLE.html#eq:fluct-score">(10.8)</a> also imply,
<em>mutatis mutandis</em>, that</p>
<p><span class="math display" id="eq:fluct-score-bis">\[\begin{equation*}  \frac{d}{d  h} \left.   L_{y}(\Qbar_{h})(O)\right|_{h=0}  =
\frac{2A      -      1}{\ell\Gbar(A,      W)}     \left(Y      -      \Qbar(A,
W)\right). \tag{C.2}\end{equation*}\]</span></p>
<p>In this light, and in view of <a href="3-smooth.html#eq:score">(3.2)</a>, we can think of <span class="math inline">\(\calQ(\Qbar, \Gbar)\)</span> as a fluctuation of <span class="math inline">\(\Qbar\)</span> in the direction of <span class="math display">\[\begin{equation*}(w,a,y)     \mapsto    \frac{2a-1}{\ell\Gbar(a,w)}     (y     -
\Qbar(a,w)).\end{equation*}\]</span> Thus if <span class="math inline">\(P \in \calM\)</span> is such that
<span class="math inline">\(\Exp_{P}(Y|A,W) = \Qbar(A,W)\)</span> and <span class="math inline">\(P(A=1|W) = \Gbar(W)\)</span>, then we can also
think of <span class="math inline">\(\calQ(\Qbar, \Gbar)\)</span> as a fluctuation of <span class="math inline">\(\Qbar\)</span> in the direction of
the second component <span class="math inline">\(D_{2}^{*}(P)\)</span> of the efficient influence curve
<span class="math inline">\(D^{*}(P)\)</span> of <span class="math inline">\(\Psi\)</span> at <span class="math inline">\(P\)</span> <a href="3-smooth.html#eq:eif">(3.4)</a>.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="B-proofs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="D-references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["tlride-book.pdf"],
"toc": {
"collapse": "section",
"scroll_hightlight": true,
"toolbar": {
"position": "static"
},
"edit": null,
"download": "pdf",
"search": true,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
}
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
