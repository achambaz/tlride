% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimation.R
\name{apply_targeting_step}
\alias{apply_targeting_step}
\title{Applies a targeting step to an initial G-computation estimator}
\usage{
apply_targeting_step(dat, Gbar, Qbar, threshold = 0.05, epsilon = NULL)
}
\arguments{
\item{dat}{The learning data  set. Must have the  same form as a  data set
produced by an object of \code{class} \code{LAW} (see '?tlrider').}

\item{Gbar}{The actual  'Gbar'  feature  of  the  law, if  it  is  known
beforehand, or an estimator  thereof, the output of \code{estimate_Gbar},
derived  by   training  an  algorithm   on  a  learning  data   set  (see
'?estimate_Gbar').}

\item{Qbar}{The output  of \code{estimate_Qbar},  derived by  training an
algorithm on a learning data set (see '?estimate_Qbar').}

\item{threshold}{A \code{numeric} in [1e-3, 1-1e-3]  (default value 1e-2),
used to bound the evaluations of 'Gbar' away from 0 and 1.}

\item{epsilon}{Either 'NULL'  (default  value) or  a \code{numeric}.   If
'epsilon' is not  'NULL', then the search for the  optimal fluctuation is
skipped and the provided value is used.}
}
\value{
A \code{tibble} containing  the value of updated estimator ('psi_n'
  column),  that  of  the  estimator of  its  standard  deviation  ('sig_n'
  column), and  the value of  the criterion \eqn{P_n  D^* (P_{n,\epsilon})}
  ('crit_n'  column). The  latter equals  approximately 0  if 'epsilon'  is
  'NULL'.
}
\description{
Given an  (initial) G-computation  estimator of \eqn{\Psi}  at the  law that
generated    the    data    set    used    to    build    the    estimator, 
\code{apply_targeting_step}  updates  it  in  one  single  step.   The
correction exploits\itemize{\item the so called  'Gbar' feature of the law,
either a  priori known or  estimated \item the  estimator of the  so called
'Qbar' feature  of the law  that was built and  used to derive  the initial
estimator \item the initial estimator \item  the data set used to infer the
above features and parameter.}
}
\details{
Caution:  the estimator of the standard deviation  of the updated
  estimator can be trusted only in very specific circumstances.
}
\examples{

## create an experiment and draw a data set from it
example(tlrider, echo = FALSE)
obs <- sample_from(experiment, n = 250)

## estimate 'QW', 'Gbar' and 'Qbar'
QW_hat <- estimate_QW(obs)
Gbar_hat <- estimate_Gbar(obs, working_model_G_one)
Qbar_hat <- estimate_Qbar(obs, working_model_Q_one)

## wrap 'Gbar_hat' and 'Qbar_hat' (two fits) into two functions
Gbar_hat_fun <- wrapper(Gbar_hat, FALSE)
Qbar_hat_fun <- wrapper(Qbar_hat, FALSE)

## compute the G-computation estimator
psi_hat <- compute_gcomp(QW_hat, Qbar_hat_fun, nrow(obs))

## apply the targeting step
(apply_targeting_step(obs, Gbar_hat_fun, Qbar_hat_fun))

}
\references{
Benkeser & Chambaz, "A Ride in Targeted Learning Territory" (2019).
}
\seealso{
\code{\link{estimate_Gbar}}   to  estimate   the   conditional
  probability that A=1 given W, \code{\link{estimate_Qbar}} to estimate the
  conditional expectation of Y  given (A,W), \code{\link{compute_gcomp}} to
  compute             the              G-computation             estimator,
  \code{\link{apply_one_step_correction}} to apply the one-step correction,
  \code{\link{wrapper}}.
}
